{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "              \n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "from DataPreprocessing import DataPreprocessing\n",
    "from FootballDataset import FootballDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-biography",
   "metadata": {},
   "source": [
    "## Football predicter\n",
    "\n",
    "### Problem: Predicting football results based on team performance\n",
    "\n",
    "In this notebook, we build, train, validate, and test a Neural Network with PyTorch to predict the __target_label__ field (win, draw, lose) of the upcoming matches\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "    * <a href='#11'> Select features to build the model </a>\n",
    "2. <a href=\"#2\">Data Processing</a>\n",
    "    * <a href=\"#21\">Data Preprocessing (cleaning)</a>\n",
    "    * <a href=\"#22\">Train - Validation - Test Datasets</a>\n",
    "    * <a href=\"#23\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "3. <a href=\"#3\">Neural Network Training and Validation</a>\n",
    "4. <a href=\"#4\">Test the Neural Network</a>\n",
    "5. <a href=\"#5\">Improvement ideas</a>\n",
    "\n",
    "\n",
    "__Rosters schema:__ \n",
    "- __id:__ (INT) Identifier of roster, which is unique for a player and match\n",
    "- __goals:__ (INT) Number of goals scored by player in that match\n",
    "- __shots:__ (INT) Number of shots to the goal for that player\n",
    "- __own_goals:__ (INT) Number of own goals scored by player in that match\n",
    "- __xG:__ (FLOAT) Expected goals for that player\n",
    "- __time:__ (INT) Field time in minutes of player\n",
    "- __player_id:__ (INT) Unique identifier for that player.\n",
    "- __team_id:__(INT) Unique identifier of that team\n",
    "- __position:__(INT) Position played by that player\n",
    "- __player:__(STR) Name of the player\n",
    "- __h_a:__(STR, ['h','a']) Home or away\n",
    "- __yellow_card:__(INT,[0,1,2]) Number of yellow cards\n",
    "- __red_card:__(INT,[0,1]) Number of red cards\n",
    "- __roster_in:__ (INT) ID of roster that substitued this player\n",
    "- __roster_out:__ (INT) ID of roster that left to give place to this player\n",
    "- __key_passes:__ (INT) number of key passes\n",
    "- __assists:__ (INT) number of assists to goals\n",
    "- __xA:__ (FLOAT) expected assists to goals\n",
    "- __xGChain:__ (FLOAT) expected goals chain\n",
    "- __xGBuildup:__ (FLOAT) expected goals buildup\n",
    "- __positionOrder:__ (INT) order in lineup position\n",
    "- __date:__ (DATE) date of match\n",
    "- __homeScore:__ (INT) Score of home team\n",
    "- __awayScore:__ (INT) Score of away team\n",
    "- __matchId:__ (INT) Unique identifier of the match\n",
    "\n",
    "__Teams schema:__\n",
    "- __matchId:__ (INT) Unique identifier of the match\n",
    "- __teamId:__ (INT) Unique identifier of the team\n",
    "- __h_a:__ (STR, ['h','a']) Home or away\n",
    "- __xG:__ (FLOAT) Expected goals for the team\n",
    "- __xGA:__ (FLOAT) Expected goals against\n",
    "- __npxG:__ (FLOAT) Expected goals for the team (excluding penalties and own goals)\n",
    "- __npxGA:__ (FLOAT) Expected goals against (excluding penalties and own goals)\n",
    "- __deep:__ (FLOAT) Passes completed within an estimated 20 yards of goal (crosses excluded)\n",
    "- __deep_allowed:__ (FLOAT) Allowed deep passes for the opposite team\n",
    "- __scored:__ (INT) Goals scored\n",
    "- __missed:__ (INT) Goals scored against\n",
    "- __xpts:__ (FLOAT) Expected points\n",
    "- __result:__ (STR, ['l','w','d']) Match result, win, draw, or loss\n",
    "- __wins:__ (BOOLEAN) True if team wins\n",
    "- __draws:__ (BOOLEAN) True if team draws\n",
    "- __loses:__ (BOOLEAN) True if team loses\n",
    "- __pts:__ (INT) Points gained for that team\n",
    "- __npxGD:__ (FLOAT) Difference between expected goals for and against, excluding penalties and own goals.\n",
    "- __ppda.att:__ (FLOAT) Passes per defensive action in the attack part of the field (PPDA metric is calculated by dividing the number of passes allowed by the defending team by the total number of defensive actions.)\n",
    "- __ppda.def:__ (FLOAT) Passes per defensive action in the defensive part of the field.\n",
    "- __ppda_allowed.att:__ (FLOAT) Opponent passes per defensive action in the attack part of the field.\n",
    "- __ppda_allowed.def:__ (FLOAT) Opponent passes per defensive action in the defensive part of the field.\n",
    "\n",
    "__Additional fields:__\n",
    "- __home_points:__ (INT) Points in season before match for home team\n",
    "- __away_points:__ (INT) Points in season before match for away team\n",
    "- __scored_goals_season_h:__ (INT) Goals scored in season for home team\n",
    "- __missed_goals_season_h:__ (INT) Goals missed in season for home team\n",
    "- __scored_goals_season_a:__ (INT) Goals scored in season for away team\n",
    "- __missed_goals_season_a:__ (INT) Goals missed in season for away team\n",
    "- __league:__ (STR) League of the match\n",
    "- __season:__ (INT) Season of the match\n",
    "- __(TO BE INSERTED)n_points_h:__ (INT) Points earned in the last N encounters for home team\n",
    "- __(TO BE INSERTED)n_points_a:__ (INT) Points earned in the last N encounters for away team\n",
    "- __(TO BE INSERTED)top_assists_h:__ (FLOAT) Highest individual season assists.\n",
    "- __(TO BE INSERTED)top_score_a:__ (FLOAT) Highest individual season score in squad\n",
    "- __avg_ppda.att_h:__(FLOAT) Season average of ppda.att for host team\n",
    "- __avg_ppda.def_h:__(FLOAT) Season average of ppda.def for host team\n",
    "- __avg_ppda.att_a:__(FLOAT) Season average of ppda.att for visiting team\n",
    "- __avg_ppda.def_a:__(FLOAT) Season average of ppda.def for visiting team\n",
    "- __avg_ppda_allowed.att_h:__(FLOAT) Season average of ppda_allowed.att for host team\n",
    "- __avg_ppda_allowed.def_h:__(FLOAT) Season average of ppda_allowed.def for host team\n",
    "- __avg_ppda_allowed.att_a:__(FLOAT) Season average of ppda_allowed.att for visiting team\n",
    "- __avg_ppda_allowed.def_a:__(FLOAT) Season average of ppda_allowed.def for visiting team \n",
    "- __avg_deep_h:__(FLOAT) Season average of deep passes for host team\n",
    "- __avg_deep_a:__(FLOAT) Season average of deep passes for visitor team\n",
    "- __avg_deep_allowed_h:__ (FLOAT) Season average of allowed deep passes for host team\n",
    "- __avg_deep_allowed_a:__ (FLOAT) Season average of allowed deep passes for visitor team\n",
    "... to be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-deployment",
   "metadata": {},
   "source": [
    "##  <a name = '#1'>0 -  Read the data set </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39204da-a53e-486f-81e9-fe8e7ced2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0. Run preprocessing\n",
    "#dataStream = DataPreprocessing()\n",
    "df = pd.read_csv('Data/Preprocessed/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a4105-2d6b-4f1b-801b-c4314f23e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = df.loc[df['season'] == 2021,:]\n",
    "def missing_pct(df):\n",
    "    # Calculate percentage of missing for each column\n",
    "    s_missing = df.isnull().sum() * 100 / df.shape[0]\n",
    "    # Convert the series back to data frame\n",
    "    df_missing = pd.DataFrame(s_missing).round(2)\n",
    "    # Reset and rename the index\n",
    "    df_missing = df_missing.reset_index().rename(\n",
    "                    columns={\n",
    "                            'index':'Column',\n",
    "                            0:'Missing_Percentage (%)'\n",
    "                    }\n",
    "                )\n",
    "    # Sort the data frame\n",
    "    df_missing = df_missing.sort_values('Missing_Percentage (%)', ascending=False)\n",
    "    return df_missing\n",
    "\n",
    "missing_pct(df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. If you need to update the dataset, uncomment below\n",
    "# dataProcess = DataPreprocessing()\n",
    "\n",
    "# Step 2. Choose params from schema below\n",
    "\"\"\"schema = [\"h_a\",\"xG\",\"xGA\",\"npxG\",\"npxGA\",\"deep\",\"deep_allowed\",\"scored\",\"missed\",\"xpts\",\"result\",\n",
    "          \"date\",\"wins\",\"draws\",\"loses\",\"pts\",\"npxGD\",\"ppda.att\",\"ppda.def\",\"ppda_allowed.att\",\n",
    "          \"ppda_allowed.def\",\"H team\",\"team_id\",\"h_a_boolean\",\"league\",\"datetime\",\"season\",\"matchId\",\n",
    "          \"home_points\",\"scored_goals_season_h\",\"missed_goals_season_h\",\"avg_ppda.att_h\",\"avg_ppda.def_h\",\n",
    "          \"avg_ppda_allowed.att_h\",\"avg_ppda_allowed.def_a\",\"avg_deep_h\",\"avg_deep_allowed_h\"]\"\"\"\n",
    "\n",
    "num_params = [\"season_points\",\"scored_goals_season\",\"missed_goals_season\",\n",
    "             \"season_points_adv\",\"scored_goals_season_adv\",\"missed_goals_season_adv\"]\n",
    "# \"avg_ppda.att\",\"avg_ppda_allowed.def\",\"avg_deep\",\"avg_deep_allowed\", \"avg_ppda.att\",\"avg_ppda_allowed.def\",\"avg_deep\",\"avg_deep_allowed\"\n",
    "\n",
    "cat_params = [\"h_a\"]\n",
    "\n",
    "target = [\"pts\"]#[\"wins\",\"draws\",\"loses\"]\n",
    "\n",
    "# Data loading\n",
    "def load_data_football(batch_size, num_par, cat_par, target):  #@save\n",
    "    \"\"\"Load the dataset into memory.\"\"\"\n",
    "    # Load dataset\n",
    "    dataset = FootballDataset(cat_par, num_par, target]#,league = [\"La liga\"], season = [2015,2016] )\n",
    "\n",
    "    # Split into training and test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Dataloaders\n",
    "    return (data.DataLoader(trainset, batch_size=batch_size, shuffle=True),\n",
    "            data.DataLoader(testset, batch_size=batch_size, shuffle=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_football(100, num_params, cat_params, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    print(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch does not implicitly reshape the inputs. Thus we define the flatten\n",
    "# layer to reshape the inputs before the linear layer in our network\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(8, 3))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4979e4-30f2-4814-bc3c-2fe8b8773123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, num_epochs, updater):  #@save\n",
    "    \"\"\"Train a model\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b3ca3-db5f-44e9-addd-7f921846988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "num_epochs = 30\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-likelihood",
   "metadata": {},
   "source": [
    "# Model 1 -  Simple Neural Network - Teams Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-sweet",
   "metadata": {},
   "source": [
    "## 1 <a name=\"#11\">Select features to build the model</a>\n",
    "(<a href=\"#1\">Go to Data Set </a>)\n",
    "\n",
    "This time we build a model using all features (except __ASIN__). That is, we build a classifier including __numerical, categorical__ and __text__ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1\n",
    "numerical_features = ['deep','ppda.att','ppda.def','npxG']#['deep','deep_allowed','ppda.att','ppda_allowed.att','ppda_allowed.def']\n",
    "categorical_features = ['h_a']\n",
    "model_features = numerical_features + categorical_features\n",
    "model_target = ['scored']#['npxG','npxGA']\n",
    "X = df.loc[:,model_features]\n",
    "X = np.array(X.values).T # Each column is a data\n",
    "Y = df.loc[:,model_target].T\n",
    "Y = np.atleast_2d(np.array(Y.values))\n",
    "\n",
    "# Set 2\n",
    "dic_switch = {'h':'a','a':'h'}\n",
    "numerical_features2 = ['deep_allowed','ppda_allowed.att','ppda_allowed.def','npxGA']\n",
    "categorical_features2 = ['h_a']\n",
    "model_features2 = numerical_features2 + categorical_features2\n",
    "model_target2 = ['missed']#['npxG','npxGA']\n",
    "df_copy = df\n",
    "df_copy['h_a'] = df_copy['h_a'].apply(lambda x: dic_switch[x])\n",
    "X2 = df_copy.loc[:,model_features2]\n",
    "X2 = np.array(X2.values).T # Each column is a data\n",
    "Y2 = df_copy.loc[:,model_target2].T\n",
    "Y2 = np.atleast_2d(np.array(Y2.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)\n",
    "print(Y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print X and X2\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-engagement",
   "metadata": {},
   "source": [
    "## 2. Data-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-klein",
   "metadata": {},
   "source": [
    "### 2.1 <a name = '#21'> Cleaning numerical features </a>\n",
    "\n",
    "Let's examine the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(numerical_features)):\n",
    "    print(df[numerical_features[i]].value_counts(bins=10, sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-detective",
   "metadata": {},
   "source": [
    "__Outliers__. We have no outlier data in the numerical features considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-christianity",
   "metadata": {},
   "source": [
    "__Missing Numerical Values__. Let's check missing values for these numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[numerical_features].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-popularity",
   "metadata": {},
   "source": [
    "There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-sixth",
   "metadata": {},
   "source": [
    "## 2.2  <a name = '#22'>Train - Validation - Test Datasets</a>\n",
    "(<a href=\"#2\">Go to Data Processing</a>)\n",
    "\n",
    "We split our dataset into training (80%), validation (10%), and test (10%) subsets using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, shuffle=True, random_state=23)\n",
    "\n",
    "# Validation-Test split\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, shuffle=True, random_state=23)\n",
    "\n",
    "# Print the shapes of the Train - Test Datasets\n",
    "print('Train - Validation - Test Datasets shapes: ', train_data.shape, val_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-religious",
   "metadata": {},
   "source": [
    "## 2.3 <a name = '#23' >Data processing with Pipeline and ColumnTransformer</a>\n",
    "\n",
    "\n",
    "We can use the composite Pipeline of Day 2 to train and tune a neural network in sklearn, using its implementation of neural network __MultiOutputRegressor__. However, sklearn is not a neural network framework, lacking access to large scale optimization techniques with GPU support and more neural network related utility functions. \n",
    " \n",
    "In a second instance, instead, we build four neural networks with __PyTorch__, one per output. While for classic, non-neural algorithms, PyTorch is not particularly useful, using an actual deep learning framework for neural network experimentation provides more flexibility and customization.\n",
    "\n",
    "Choice of model and hosting platform aside, we can still reuse the collective ColumnTransformer from Day 2 to preprocess the data for neural network training, validation and test, ensuring that the transformations learned on the train data are performed accordingly on the training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) # Shown in case is needed, not a must with Decision Trees\n",
    "                                ])\n",
    "                  \n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline([\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Shown in case is needed, no effect here as we already imputed with 'nan' strings\n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')) # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "                                ])\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_processor = ColumnTransformer([\n",
    "    ('numerical_processing', numerical_processor, numerical_features),\n",
    "    ('categorical_processing', categorical_processor, categorical_features)\n",
    "                                    ]) \n",
    "\n",
    "# Visualize the data processing pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "data_processor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PROCESSING ###\n",
    "#######################\n",
    "\n",
    "# Get train data to train the network\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target].values\n",
    "\n",
    "# Get validation data to validate the network \n",
    "X_val = val_data[model_features]\n",
    "y_val = val_data[model_target].values\n",
    "\n",
    "# Get test data to test the network for submission to the leaderboard\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target].values\n",
    "\n",
    "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_processor.fit_transform(X_train)#.toarray()\n",
    "X_val = data_processor.transform(X_val)#.toarray()\n",
    "X_test = data_processor.transform(X_test)#.toarray()\n",
    "\n",
    "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-array",
   "metadata": {},
   "source": [
    "## 3.1 Multi Output Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = MultiOutputRegressor(LinearRegression()).fit(X_train, y_train)\n",
    "\n",
    "print(f'Example: Multi: Predict = {multi.predict(X_train[[2]])[0][0]:.2f}, Actual = {y_train[2][0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-fashion",
   "metadata": {},
   "source": [
    "## 3.2 MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "MLP = MLPRegressor(random_state=1, alpha = 0.01, max_iter=50000, hidden_layer_sizes = (100,50)).fit(X_train, y_train.ravel())\n",
    "print(f'The Rˆ2 for the training set is = {MLP.score(X_train, y_train):.3f}')\n",
    "print(f'The Rˆ2 for the test set is = {MLP.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1\n",
    "X = df[model_features]\n",
    "X = data_processor.fit_transform(X)\n",
    "Y = df[model_target].values\n",
    "\n",
    "# Set 2\n",
    "columns_ = ['deep_allowed','ppda_allowed.att','ppda_allowed.def','npxGA','h_a']\n",
    "dic_switch = {'h':'a','a':'h'}\n",
    "cols_rename = {'deep_allowed': 'deep','ppda_allowed.att':'ppda.att', \n",
    "              'ppda_allowed.def':'ppda.def','npxGA':'npxG','h_a':'h_a','missed':'scored'}\n",
    "df_copy = df[cols_rename.keys()]\n",
    "df_copy.loc[:,'h_a'] = df_copy['h_a'].apply(lambda x: dic_switch[x])\n",
    "df_copy.rename(columns = cols_rename, inplace = True)\n",
    "\n",
    "X2 = df_copy[model_features]\n",
    "X2 = data_processor.fit_transform(X2) # Each column is a data\n",
    "Y2 = df_copy[model_target].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'Predicted xG': MLP.predict(X),\n",
    "       'Actual xG': Y.flatten(),\n",
    "       'Predicted xGA': MLP.predict(X2),\n",
    "       'Actual xGA': Y2.flatten()}\n",
    "df_output = pd.DataFrame(dic)\n",
    "\n",
    "def result(G1,G2):\n",
    "    if (G1-G2) > 0.5:\n",
    "        return 'w'\n",
    "    elif (G1-G2) < -0.5:\n",
    "        return 'l'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "\n",
    "df_output['Predicted xResult'] = df_output.apply(lambda x: result(x['Predicted xG'],x['Predicted xGA']),axis = 1)\n",
    "df_output['Actual xResult'] = df_output.apply(lambda x: result(x['Actual xG'],x['Actual xGA']),axis = 1)\n",
    "perc = df_output.apply(lambda x: x['Predicted xResult'] == x['Actual xResult'],axis=1).sum()/len(df_output)*100\n",
    "print(f'The result accuracy is {perc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-egypt",
   "metadata": {},
   "source": [
    "\n",
    "## ML Model with PyTorch\n",
    "Here we implement the training with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-doubt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "warming-management",
   "metadata": {},
   "source": [
    "## 2 - ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ML parameters\n",
    "n_x = X.shape[0]\n",
    "n_y = Y.shape[0]\n",
    "n_h = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = (A,Z)\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = (A,Z)\n",
    "    return A, cache\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A) + b \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    A, Z = cache\n",
    "    dZ = dA * A * (1 - A)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    A, Z = cache\n",
    "    dZ = np.multiply(dA, np.int64(A > 0))\n",
    "    return dZ\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    assert dA_prev.shape == A_prev.shape\n",
    "    assert dW.shape == W.shape\n",
    "    assert db.shape == b.shape\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "    \n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    Z1 = np.dot(W1,X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    [Note that the parameters argument is not used in this function, \n",
    "    but the auto-grader currently expects this parameter.\n",
    "    Future version of this notebook will fix both the notebook \n",
    "    and the auto-grader so that `parameters` is not needed.\n",
    "    For now, please include `parameters` in the function signature,\n",
    "    and also when invoking this function.]\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = -np.squeeze(np.dot(np.log(A2),Y.T)) - np.squeeze(np.dot((1-Y),np.log(1-A2).T))\n",
    "    cost = 1/m *logprobs\n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m*np.dot(dZ2,A1.T)\n",
    "    db2 = 1/m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = 1/m*np.dot(dZ1,X.T)\n",
    "    db1 = 1/m*np.sum(dZ1,axis = 1, keepdims = True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = W1-learning_rate*dW1\n",
    "    b1 = b1-learning_rate*db1\n",
    "    W2 = W2-learning_rate*dW2\n",
    "    b2 = b2-learning_rate*db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1,'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2,'sigmoid')\n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2,Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2,cache2,'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1,'relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = (n_x,n_h,n_y)\n",
    "params = two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-return",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-floor",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-replica",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "d2l:Python",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
